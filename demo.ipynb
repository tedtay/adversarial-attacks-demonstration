{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import warnings\n",
    "\n",
    "\n",
    "# to ignore torchvision model warnings\n",
    "warnings.filterwarnings(\n",
    "    action='ignore',\n",
    "    category=DeprecationWarning,\n",
    "    \n",
    "    module=r'.*randpool'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# **Overview**\n",
    "<font size=\"3\">\n",
    "This Notebook aims to demonstrate one of the inherent weaknesses of using convolutional neural networks (CNN's) in safety critical systems such as security systems, autonomous driving applications and more. The notebook offers a walk through of the steps and code necessary for you to understand the concepts involved and to try out the idea for yourself. Please feel free to re-use any code you see in this document.\n",
    "\n",
    "* <pre> demo.ipynb   ->  demonstration notebook to follow along</pre> \n",
    "*  <pre> image.py     ->  supporting python file that provides the Image class and additional functionality I created</pre> \n",
    "</br>\n",
    "</font>\n",
    "\n",
    "\n",
    "\n",
    "## The weakness\n",
    "<font size=\"3\">**The Architecture:**\n",
    "CNN's and NN's rely on precise weights that have been calibrated through the propagation of training errors to correct these prediction weights that are stored in the neurons of the hidden layers in the networks.</font>\n",
    "</br></br>\n",
    "<font size=\"3\">**The Hypotheses:**\n",
    "If we had access the the model (and therefore the neuron weights) used in these safety critical systems we could potential exploit their over reliance on particular neurons and their associated weights to adjust the model predictions to our benefit.</font>\n",
    "</br></br>\n",
    "<font size=\"3\">**The Approach:**\n",
    "This notebook uses the famous ResNet-18 model as an example of how these weights can be used to our advantage as well as some new arbitrary and unseen images for the model to make predictions on.</font>\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from PIL import Image"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "<font size=\"3\">We will be using the Python Imaging Library (PIL) to make precise changes to our images</font>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "our_banana_raw = Image.open('/Users/ted.taylor/Downloads/ban.jpg')\n",
    "print(f\"Original size : {our_banana_raw.size}\") # 5464x3640\n",
    "\n",
    "our_banana = our_banana_raw.resize((256, 256))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "<font size=\"3\">Now we need to import an example image for the model to make predictions on, in this case our example image is just a banana</font>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from image import Image, pred\n",
    "\n",
    "# to ignore torchvision model warnings\n",
    "warnings.filterwarnings(action='ignore',category=DeprecationWarning,module=r'.*randpool')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "<font size=\"3\">We should also import our supporting python file  (and the pred method) which contains many of the supporting methods I created </font>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "our_image = Image(our_banana)\n",
    "our_image.matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "<font size=\"3\">We can then create an Image object by passing in our image we loaded, we can then view this image using the .matrix function on our Image object</font>\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"{len(pred(our_image.matrix))}\\n\")\n",
    "pred(our_image.matrix,5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "<font size=\"3\">By passing our image to the pred() function we can see it returns a list containing 1000 elements, we can also pass in an integer to limit the results to the top 'n' predictions, in this case we only really want to look at the top 5</font>\n",
    "</br> </br> \n",
    "<font size=\"3\">**As we can see the ResNet-18 model stored within the Image class is 99.9% sure our image is a banana!** </font>\n",
    "</br> </br> \n",
    "<font size=\"3\"> Lets aim to trick the model into thinking this banana is a 'golf ball' to the point where it is 99% confident</font>\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "white_image = our_banana_raw.resize((256, 256))\n",
    "for i in range(0,255):\n",
    "    for j in range(0,255):\n",
    "        white_image.load()[i,j] = (255,255,255)\n",
    "black_image = our_banana_raw.resize((256, 256))\n",
    "for i in range(0,255):\n",
    "    for j in range(0,255):\n",
    "        black_image.load()[i,j] = (0,0,0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2d2e8d-5196-4a0e-b36f-285367d6ae2f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea503d3c-0baa-4a08-8e9c-fbc64e18446b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "white_image = our_banana_raw.resize((256, 256))\n",
    "for i in range(0,255):\n",
    "    for j in range(0,255):\n",
    "        white_image.load()[i,j] = (255,255,255)\n",
    "black_image = our_banana_raw.resize((256, 256))\n",
    "for i in range(0,255):\n",
    "    for j in range(0,255):\n",
    "        black_image.load()[i,j] = (0,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ddcb7a-110a-4b6e-8bc2-550c27721201",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec99a57-5fda-4386-9728-3e4e7dc5719e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81355ef2-926b-4798-ba99-d19a1e46492a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef009927-3ddb-45af-b135-26cacf8664b8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3077a6d7-18b2-41f1-a8c4-8c966a6b8db9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352623aa-42a5-4f35-9037-2584a8810941",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e03f0845-80ed-46e1-bdb7-745ffd92ad2c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def imageResize(image_param):\n",
    "    \n",
    "    from torchvision import transforms\n",
    "    #\n",
    "    # Create a preprocessing pipeline\n",
    "    #\n",
    "    preprocess = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )])\n",
    "    #\n",
    "    # Pass the image for preprocessing and the image preprocessed\n",
    "    #\n",
    "    img_cat_preprocessed = preprocess(image_param)\n",
    "    return img_cat_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12c156fc-e245-43b7-bd6d-a69fbb520e64",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pred(image_param):\n",
    "    img_cat_preprocessed = imageResize(image_param)\n",
    "    #\n",
    "    # Reshape, crop, and normalize the input tensor for feeding into network for evaluation\n",
    "    #\n",
    "    batch_img_cat_tensor = torch.unsqueeze(img_cat_preprocessed, 0)\n",
    "\n",
    "    model.eval()\n",
    "    #\n",
    "    # Get the predictions of image as scores related to how the loaded image\n",
    "    # matches with 1000 ImageNet classes. The variable, out is a vector of 1000 scores\n",
    "    #\n",
    "    out = model(batch_img_cat_tensor)\n",
    "\n",
    "    # Load the file containing the 1,000 labels for the ImageNet dataset classes\n",
    "    #\n",
    "    with open('/Users/ted.taylor/Downloads/imagenet_classes.txt') as f:\n",
    "        labels = [line.strip() for line in f.readlines()]\n",
    "    #\n",
    "    # Find the index (tensor) corresponding to the maximum score in the out tensor.\n",
    "    # Torch.max function can be used to find the information\n",
    "    #\n",
    "    _, index = torch.max(out, 1)\n",
    "    #\n",
    "    # Find the score in terms of percentage by using torch.nn.functional.softmax function\n",
    "    # which normalizes the output to range [0,1] and multiplying by 100\n",
    "    #\n",
    "    percentage = torch.nn.functional.softmax(out, dim=1)[0] * 100\n",
    "    #\n",
    "    # Print the name along with score of the object identified by the model\n",
    "    #\n",
    "    #print(labels[index[0]], percentage[index[0]].item())\n",
    "    #\n",
    "    # Print the top 5 scores along with the image label. Sort function is invoked on the torch to sort the scores.\n",
    "    #\n",
    "    _, indices = torch.sort(out, descending=True)\n",
    "    preds = [(labels[idx], percentage[idx].item()) for idx in indices[0][:]]\n",
    "    \n",
    "    return (preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70e98a77-5408-4aef-af65-be942a00f8a4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563f15dc-dd9d-409f-97bd-629e332da8e7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc9f4a1-f597-4160-b756-a3d4a12e3188",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a01d65-3779-45c6-a825-a39c73639633",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cfbe73-4aad-4ccc-b9d6-11822eba1de1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a62b0f-1188-40d4-b7f3-b51c5e0ae6a9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "result = trick(og_image.resize((256, 256)),'eggnog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a993f81-9562-409c-b89f-a3147012370d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d029ee70-e801-48af-b915-6b28c073b091",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd654612-6bf5-43ce-88cc-d9ae62cf36d1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4435ae-c92d-4958-82a8-09f17b7e265b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02512e70-60d3-4f62-9545-0ce5811fcbee",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7f43ee-276d-438f-b87b-b78c2fdbcd68",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e975a8-e8c5-4def-a014-209f508b55ee",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1426d641-ad25-4858-a1cd-d9090107aa64",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850c840d-7203-4269-9b3f-fad1d4dca1ad",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbe3998-396d-4dad-9120-35f6940e64e7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbd8fa1-2aab-486a-9b2d-42d95d1e179e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3698391-fc8a-431c-931b-8978894fc6de",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Thoughts on improvements ?\n",
    "<br>- Count what colors and sizes using in plots - once it gets to 25% restart with most common color-size weights increased proba\n",
    "<br>- use k counter to nudge out of a local minimum - create thick white border around edge of image to nudge along\n",
    "<br>- total pixel value count - if less than 1/4 of pixels != 255,255,255 at 25% restart process (avoids creating small objects/ noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6101ebae-f27a-4bf6-a036-bd38df4daa19",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a1bad6-1e66-4994-a99c-ea0d55ca4c73",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a398a0-490a-4ef9-b205-403a324362ef",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd3acad-e546-43ad-a557-f17975289046",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9998d10-ba5c-450d-b762-f19efb42081b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5625b81-9492-4f86-8af3-84b50cd31cd7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a028f217-6a86-4600-96a9-02fdd3366f07",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f7dfc2-0a2a-41c2-b93c-65f0c74a4026",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65886b6-8325-4d15-b04b-04bcb729c5f2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74292430-326c-447f-a301-9f9bc19da6b8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1127ae-eef3-4499-9600-32ae8e75e624",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920b5e1a-39c4-4f9d-bd65-ee8fc6720db9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envAltair",
   "language": "python",
   "name": "envaltair"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}